{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded6c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSRRFF\n",
    "import math, random, numpy as np, os, warnings, pickle, re\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Iterable, List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use(\"Agg\")  # headless\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    PANDAS_OK = True\n",
    "except Exception:\n",
    "    PANDAS_OK = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42); np.random.seed(42); random.seed(42)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805fb303",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(data, x_len, y_len, gap=1):\n",
    "    \"\"\"\n",
    "        Generates input-output pairs for multi-step time series forecasting.\n",
    "        Args:\n",
    "            data (np.ndarray): Array of shape (T, n_features) representing the time series.\n",
    "            x_len (int): Length of the input window (number of past time steps).\n",
    "            y_len (int): Length of the output window (forecast horizon).\n",
    "            gap (int, optional): Number of time steps between input and output windows. Default is 1.\n",
    "\n",
    "        Returns:\n",
    "            x (np.ndarray): Array of input windows with shape (num_samples, x_len, n_features).\n",
    "            y (np.ndarray): Array of output windows with shape (num_samples, y_len, n_features).\n",
    "\n",
    "        Each sample consists of an input sequence (x) and a target sequence (y), separated by a gap.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    length = data.shape[0]\n",
    "    for end_idx in range(x_len + y_len + gap, length):\n",
    "        xtime = data[end_idx-y_len-x_len-gap:end_idx-y_len-gap]\n",
    "        ytime = data[end_idx-y_len:end_idx]\n",
    "        x.append(xtime)\n",
    "        y.append(ytime)\n",
    "    x = np.stack(x)\n",
    "    y = np.stack(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3b554",
   "metadata": {},
   "source": [
    "# **Wind speed synthetic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc62901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_zscore(y: np.ndarray, X: np.ndarray, eps: float = 1e-6):\n",
    "    \"\"\"NaN/Inf-aware z-score with std clamping.\"\"\"\n",
    "    # Replace inf with NaN for stats, keep copies\n",
    "    y = y.astype(float, copy=True)\n",
    "    X = X.astype(float, copy=True)\n",
    "    y[~np.isfinite(y)] = np.nan\n",
    "    X[~np.isfinite(X)] = np.nan\n",
    "\n",
    "    y_mu = np.nanmean(y)\n",
    "    y_sd = np.nanstd(y)\n",
    "    if not np.isfinite(y_mu): y_mu = 0.0\n",
    "    if (not np.isfinite(y_sd)) or (y_sd < eps): y_sd = eps\n",
    "    yz = (y - y_mu) / y_sd\n",
    "\n",
    "    X_mu = np.nanmean(X, axis=0, keepdims=True)\n",
    "    X_sd = np.nanstd(X, axis=0, keepdims=True)\n",
    "    X_mu = np.where(np.isfinite(X_mu), X_mu, 0.0)\n",
    "    X_sd = np.where((~np.isfinite(X_sd)) | (X_sd < eps), eps, X_sd)\n",
    "    Xz = (X - X_mu) / X_sd\n",
    "\n",
    "    # Final sweep to guarantee finiteness\n",
    "    yz = np.nan_to_num(yz, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    Xz = np.nan_to_num(Xz, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return yz, Xz, (float(y_mu), float(y_sd), X_mu.astype(float), X_sd.astype(float))\n",
    "\n",
    "def windsynth_pp(T=24*240, test_shift=False,seed=123):\n",
    "    t = np.arange(T, dtype=float)\n",
    "\n",
    "    # base components\n",
    "    np.random.seed(seed)\n",
    "    diurnal   = 2.5 + 1.5*np.sin(2*np.pi*t/24 + 0.3*np.sin(2*np.pi*t/168))\n",
    "    sub_daily = 0.6*np.sin(2*np.pi*t/6 + 0.5) + 0.4*np.sin(2*np.pi*t/3 + 1.3)\n",
    "    weekly    = 0.8*np.sin(2*np.pi*t/168 + 0.2)\n",
    "    base = diurnal + sub_daily + weekly\n",
    "\n",
    "    # regimes\n",
    "    cp1, cp2 = int(0.35*T), int(0.65*T)\n",
    "    regime = np.ones(T, dtype=float)\n",
    "    regime[cp1:cp2] = 1.2\n",
    "    regime[cp2:]    = 0.9\n",
    "\n",
    "    # heteroskedastic noise\n",
    "    eps = 0.3*(0.5 + 0.5*np.abs(np.sin(2*np.pi*t/24))) * np.random.randn(T)\n",
    "\n",
    "    # raw signal\n",
    "    y = regime*base + eps\n",
    "\n",
    "    # optional distribution shift\n",
    "    if test_shift:\n",
    "        i_va = int(0.8*T)\n",
    "        t2 = np.arange(T - i_va, dtype=float)\n",
    "        diurnal_shift = 2.5 + 1.8*np.sin(2*np.pi*t2/24 + 0.4*np.sin(2*np.pi*t2/168))\n",
    "\n",
    "        seg = (y[i_va:] - diurnal[i_va:]) + diurnal_shift #+ 0.5*bursts_shift\n",
    "        y[i_va:] = np.nan_to_num(seg, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # exogenous cyclical time features\n",
    "    hour = (t % 24)/24.0\n",
    "    week = (t % 168)/168.0\n",
    "    X = np.stack([\n",
    "        np.sin(2*np.pi*hour), np.cos(2*np.pi*hour),\n",
    "        np.sin(2*np.pi*week), np.cos(2*np.pi*week)\n",
    "    ], axis=-1)\n",
    "\n",
    "\n",
    "    return y, X\n",
    "\n",
    "class WindowDS(Dataset):\n",
    "    def __init__(self, y, X, W=96, H=24, stride=1, zscore=True):\n",
    "        self.y = np.nan_to_num(np.asarray(y, dtype=float), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        self.X = np.nan_to_num(np.asarray(X, dtype=float), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        self.W, self.H, self.stride = int(W), int(H), int(stride)\n",
    "        self.idxs = list(range(0, len(self.y)-(self.W+self.H)+1, self.stride))\n",
    "\n",
    "        if zscore:\n",
    "            self.yz, self.Xz, stats = safe_zscore(self.y, self.X, eps=1e-6)\n",
    "            self.y_mu, self.y_sd, self.X_mu, self.X_sd = stats\n",
    "        else:\n",
    "            self.yz, self.Xz = self.y, self.X\n",
    "            self.y_mu, self.y_sd = 0.0, 1.0\n",
    "            self.X_mu = np.zeros((1, self.X.shape[1]), dtype=float)\n",
    "            self.X_sd = np.ones((1, self.X.shape[1]), dtype=float)\n",
    "\n",
    "        # final guarantees\n",
    "        self.yz = np.nan_to_num(self.yz, 0.0, 0.0, 0.0).astype(np.float32)\n",
    "        self.Xz = np.nan_to_num(self.Xz, 0.0, 0.0, 0.0).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s = self.idxs[i]\n",
    "        xin = np.concatenate([\n",
    "            self.yz[s:s+self.W, None],           # (W,1)\n",
    "            self.Xz[s:s+self.W, :]               # (W,4)\n",
    "        ], axis=-1)\n",
    "        yout = self.y[s+self.W:s+self.W+self.H]\n",
    "\n",
    "        # safety assert (optional, helps debugging early)\n",
    "        if (not np.isfinite(xin).all()) or (not np.isfinite(yout).all()):\n",
    "            raise ValueError(f\"Non-finite values at sample {i}, \"\n",
    "                             f\"win [{s}:{s+self.W}], hor [{s+self.W}:{s+self.W+self.H}]\")\n",
    "\n",
    "        return torch.from_numpy(xin).float(), torch.from_numpy(yout.astype(np.float32))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Datos Sintéticos\n",
    "# ========================================\n",
    "\n",
    "# Generar datos\n",
    "y, X_exog = windsynth_pp(T=24*240, test_shift=True, seed=123)\n",
    "\n",
    "# Definir splits\n",
    "N = len(y)\n",
    "i_tr, i_va, i_te = int(0.7*N), int(0.8*N), N\n",
    "\n",
    "# Normalizar y preparar\n",
    "max_synth = np.max(y)\n",
    "y_normalized = y / max_synth\n",
    "\n",
    "# Crear ventanas\n",
    "tau, tau_ = 96, 24\n",
    "X_synth, Y_synth = build_data(y_normalized, tau, tau_)\n",
    "\n",
    "# GUARDAR en data_dict\n",
    "data_dict[\"Synthetic\"] = {\n",
    "    'X': X_synth, \n",
    "    'Y': Y_synth, \n",
    "    \"time_series\": y,\n",
    "    \"Max\": max_synth\n",
    "}\n",
    "\n",
    "print(f\"Synthetic - X: {X_synth.shape}, Y: {Y_synth.shape}, Max: {max_synth:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAR (usando variables locales)\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "ax.plot(range(0, i_tr), y[:i_tr], color='tab:blue', linewidth=1.2, label='Train', alpha=0.9)\n",
    "ax.plot(range(i_tr, i_va), y[i_tr:i_va], color='tab:orange', linewidth=1.2, label='Validation', alpha=0.9)\n",
    "ax.plot(range(i_va, i_te), y[i_va:i_te], color='tab:green', linewidth=1.2, label='Test', alpha=0.9)\n",
    "ax.axvline(x=i_tr, color='red', linestyle='--', linewidth=1.5, alpha=0.6)\n",
    "ax.axvline(x=i_va, color='red', linestyle='--', linewidth=1.5, alpha=0.6)\n",
    "\n",
    "\n",
    "ax.set_xlabel('Time (hours)', fontsize=20)\n",
    "ax.set_ylabel('Wind Speed', fontsize=20)\n",
    "ax.tick_params(axis='both', labelsize=20)\n",
    "\n",
    "# Eje X restringido a [0, N]\n",
    "n_samples = len(y)\n",
    "ax.set_xlim(0, n_samples - 1)\n",
    "\n",
    "# Leyenda abajo con 3 columnas\n",
    "ax.legend(\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, -0.18),\n",
    "    ncol=3,\n",
    "    fontsize=20,\n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "ax.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a01cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89494796",
   "metadata": {},
   "source": [
    "# **Wind speed real**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff117f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = Path(\"../data/raw\")\n",
    "dataSets = list(raw_path.rglob(\"*.*\"))\n",
    "\n",
    "dataSets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_mapping = {\n",
    "    \"dataset.pkl\": \"Netherlands\",\n",
    "    \"Chengdu_Airport_China.txt\": \"Chengdu\",\n",
    "    \"Argone_IL.txt\": \"Argonne\",\n",
    "    \"Beijing_Airport_China.txt\": \"Beijing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b57d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 0 \n",
    "for file_path in dataSets:\n",
    "    file_names = file_path.name\n",
    "    label = file_names[-3:]\n",
    "    name_DB = name_mapping.get(file_names, \"Unknown\")\n",
    "    flag += 1\n",
    "\n",
    "    print(f\"\\n Read data ------------------------------ \\n\\t  {file_names}\")\n",
    "\n",
    "    if label == \"csv\":        \n",
    "        df = pd.read_csv(file_path)\n",
    "        col = None\n",
    "        if 'wind_speed' in df.columns:\n",
    "            col = 'wind_speed'\n",
    "            tau  = 5\n",
    "            tau_ = 1\n",
    "        elif 'Speed' in df.columns:\n",
    "            col = 'Speed'\n",
    "            tau  = 15\n",
    "            tau_ = 7\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        values = df[col]\n",
    "        n_nan = values.isnull().sum()\n",
    "\n",
    "        max_ = max(values)\n",
    "        print(max_)\n",
    "        values_normalized = values/max_\n",
    "\n",
    "        if n_nan == 0:\n",
    "            X, Y = build_data(values_normalized, tau, tau_)\n",
    "            fragment_to_use = values_normalized\n",
    "            print(X.shape, Y.shape)\n",
    "            data_dict[name_DB] = {'X': X, 'Y': Y, \"time_series\": values, \"Max\": max_}\n",
    "\n",
    "        else:\n",
    "            mask = values.notna()\n",
    "            group = (mask != mask.shift()).cumsum()\n",
    "            non_nan_groups = values[mask].groupby(group[mask])\n",
    "\n",
    "            group_sizes = [(g, len(non_nan_groups.get_group(g))) for g in non_nan_groups.groups]\n",
    "\n",
    "            if len(group_sizes) > 0:\n",
    "                longest_idx, longest_segment = max(group_sizes, key=lambda x: x[1])\n",
    "                values = non_nan_groups.get_group(longest_idx).to_numpy()\n",
    "\n",
    "                if len(values) > (tau + tau_):\n",
    "                    X, Y = build_data(values, tau, tau_)\n",
    "                    print(X.shape, Y.shape, max_)\n",
    "                    data_dict[name_DB] = {'X': X, 'Y': Y, \"time_series\": values, \"Max\": max_}\n",
    "        \n",
    "        # ✨ NUEVO ESTILO DE VISUALIZACIÓN\n",
    "        fig, ax = plt.subplots(figsize=(14, 5))\n",
    "        ax.plot(values, linewidth=0.6, color='steelblue', alpha=0.8)\n",
    "        ax.set_xlabel('Time steps', fontsize=12)\n",
    "        ax.set_ylabel('Wind Speed', fontsize=12)\n",
    "        ax.set_title(f'{name_DB} - Wind Speed Time Series', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    elif label == \"txt\":\n",
    "        tau  = 20\n",
    "        tau_ = 7\n",
    "        if file_names == \"Argone_IL.txt\":\n",
    "            df = pd.read_csv(file_path, sep=\"\\t\",  names=['yyyymmdd','hhmn','mph','code'])\n",
    "            df = df[df[\"yyyymmdd\"] >= 19980101]\n",
    "            df[\"sped\"] = pd.to_numeric(df[\"mph\"], downcast=\"float\")\n",
    "            timeSerie = df.iloc[:,2].values\n",
    "        elif file_names == \"Beijing_Airport_China.txt\":\n",
    "            df = pd.read_csv(file_path, delimiter=\",\")\n",
    "            df[\"sped\"] = pd.to_numeric(df[\"sped\"], downcast=\"float\")\n",
    "            timeSerie = df.iloc[:,1].values\n",
    "        elif file_names == \"Chengdu_Airport_China.txt\":    \n",
    "            df = pd.read_csv(file_path, sep=\",\")\n",
    "            df[\"sped\"] = pd.to_numeric(df[\"sped \"], downcast=\"float\")\n",
    "            timeSerie = df.iloc[:,2].values\n",
    "        \n",
    "        max_ = max(timeSerie)\n",
    "        print(max_)\n",
    "        timeSerie_normalized = timeSerie/max_\n",
    "        X, Y = build_data(timeSerie_normalized, tau, tau_)\n",
    "        print(X.shape, Y.shape)\n",
    "        data_dict[name_DB] = {'X': X, 'Y': Y, \"time_series\": timeSerie, \"Max\": max_}\n",
    "\n",
    "        # ✨ NUEVO ESTILO DE VISUALIZACIÓN\n",
    "        fig, ax = plt.subplots(figsize=(14, 5))\n",
    "        ax.plot(timeSerie, linewidth=0.6, color='steelblue', alpha=0.8)\n",
    "        ax.set_xlabel('Time steps', fontsize=12)\n",
    "        ax.set_ylabel('Wind Speed', fontsize=12)\n",
    "        ax.set_title(f'{name_DB} - Wind Speed Time Series', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    elif label == \"pkl\":\n",
    "        tau  = 10\n",
    "        tau_ = 10\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        # data: Timesteps x Cities x Features        \n",
    "        i_wind = 0  \n",
    "\n",
    "        for city in range(3):\n",
    "            values = np.concatenate([data['train'][:, city, i_wind], data['test'][:, city, i_wind]]) \n",
    "            max_ = max(values)\n",
    "\n",
    "            values_normalized = values/max_\n",
    "\n",
    "            print(max(values))\n",
    "    \n",
    "            X, Y = build_data(values_normalized, tau, tau_)\n",
    "            print(X.shape, Y.shape)\n",
    "            data_dict[f\"{name_DB}-{city}\"] = {'X': X, 'Y': Y, \"time_series\": values, \"Max\": max_}\n",
    "\n",
    "            # ✨ NUEVO ESTILO DE VISUALIZACIÓN\n",
    "            fig, ax = plt.subplots(figsize=(14, 5))\n",
    "            ax.plot(values, linewidth=0.6, color='steelblue', alpha=0.8)\n",
    "            ax.set_xlabel('Time steps', fontsize=12)\n",
    "            ax.set_ylabel('Wind Speed', fontsize=12)\n",
    "            ax.set_title(f'{name_DB} City {city} - Wind Speed Time Series', fontsize=14, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d257b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf7ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(\"../data/processed/data_dict.pkl\")\n",
    "\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(data_dict, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
